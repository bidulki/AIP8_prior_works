{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMfaWq12uUxv5kZuEl9ENi9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#Step 0.0: Mount Google Drive in Colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0vbE7TOxsK0","executionInfo":{"status":"ok","timestamp":1701071114900,"user_tz":-540,"elapsed":2422,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"896052ce-d054-4b22-dad3-8dffc4bec270"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Step 0.1: Install Dependencies\n","\n","!pip install nltk transformers torch\n","import nltk\n","nltk.download('punkt')"],"metadata":{"id":"rTyjqddo6bz6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701071124293,"user_tz":-540,"elapsed":7646,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"0846ae2f-c28c-4010-9f58-85696faf9314"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#Step 0.2: Import Libraries\n","\n","import os\n","import json\n","import random\n","import time\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch.cuda.amp import GradScaler, autocast # for mixed precision\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from nltk.tokenize import sent_tokenize\n"],"metadata":{"id":"KZ0Xw6bI0cNl","executionInfo":{"status":"ok","timestamp":1701071136330,"user_tz":-540,"elapsed":8573,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Step 0.3 Check Device\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmOi6qFFO5kV","executionInfo":{"status":"ok","timestamp":1701071146712,"user_tz":-540,"elapsed":334,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"f7c6de10-59af-4a43-eaf3-7794351dfe84"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["folder_path = '/content/drive/My Drive/20per'\n","file_count = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])\n","print(f\"Number of files in the folder: {file_count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GI8Kpa-Qslh","executionInfo":{"status":"ok","timestamp":1701063293740,"user_tz":-540,"elapsed":842,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"cc387380-5edb-4f9f-9e63-726aa7607583"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of files in the folder: 7365\n"]}]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"_qJL7Q_pRAzE"}},{"cell_type":"code","source":["# Function to extract data from a JSON file and derive labels\n","def extract_data_from_json(file_path):\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","        passage = data[\"Meta(Refine)\"][\"passage\"]\n","        summary = data[\"Annotation\"][\"summary3\"]\n","        passage_sentences = sent_tokenize(passage)\n","        summary_sentences = set(sent_tokenize(summary))\n","        labels = [1 if sentence in summary_sentences else 0 for sentence in passage_sentences]\n","        return passage_sentences, labels\n","\n","# Function to prepare data\n","def prepare_data(file_names, folder_path, num_files, split_ratio=0.8):\n","    sampled_files = random.sample(file_names, num_files)\n","    train_files = sampled_files[:int(len(sampled_files) * split_ratio)]\n","    test_files = sampled_files[int(len(sampled_files) * split_ratio):]\n","\n","    train_passages, train_labels = [], []\n","    test_passages, test_labels = [], []\n","\n","    for file_name in train_files:\n","        file_path = os.path.join(folder_path, file_name)\n","        passage_sentences, labels = extract_data_from_json(file_path)\n","        train_passages.extend(passage_sentences)\n","        train_labels.extend(labels)\n","\n","    for file_name in test_files:\n","        file_path = os.path.join(folder_path, file_name)\n","        passage_sentences, labels = extract_data_from_json(file_path)\n","        test_passages.extend(passage_sentences)\n","        test_labels.extend(labels)\n","\n","    return train_passages, train_labels, test_passages, test_labels\n","\n","# Dataset class for text summarization\n","class TextSummarizationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","        encoding = self.tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","          truncation=True\n","        )\n","        return {\n","          'text': text,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Load tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Data preparation\n","folder_path = '/content/drive/My Drive/20per'  # Data folder\n","file_names = os.listdir(folder_path)\n","num_files = int(len(file_names) * 0.02)  # Use 0.2% of the files (from total 7365 files)\n","\n","train_passages, train_labels, test_passages, test_labels = prepare_data(file_names, folder_path, num_files)\n","train_dataset = TextSummarizationDataset(train_passages, train_labels, tokenizer)\n","test_dataset = TextSummarizationDataset(test_passages, test_labels, tokenizer)\n","\n","train_data_loader = DataLoader(train_dataset, batch_size=2)\n","test_data_loader = DataLoader(test_dataset, batch_size=2)"],"metadata":{"id":"v3MUxQ_TQyDU","executionInfo":{"status":"ok","timestamp":1701071761722,"user_tz":-540,"elapsed":119834,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"yJuBkE1cDtYQ"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"Oad1IGNCxKak","executionInfo":{"status":"ok","timestamp":1701072055163,"user_tz":-540,"elapsed":286408,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"08f2f05e-1b7f-4ad1-8444-e2ce15005703"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch [1/1]: Loss: 0.4716, Accuracy: 0.8330\n","Training completed in 283.02 seconds\n"]}],"source":["# Load model\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n","model = model.to(device)\n","\n","# Training loop\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","num_epochs = 1\n","total_steps = len(train_data_loader) * num_epochs\n","\n","start_time = time.time()\n","\n","print(\"Starting training...\")\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for i, batch in enumerate(train_data_loader):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        logits = outputs.logits\n","        preds = torch.argmax(logits, dim=1)\n","        correct_predictions += (preds == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    epoch_loss = total_loss / len(train_data_loader)\n","    epoch_accuracy = correct_predictions / total_predictions\n","    print(f\"Epoch [{epoch+1}/{num_epochs}]: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","training_time = time.time() - start_time\n","print(f\"Training completed in {training_time:.2f} seconds\")"]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"pKUxZyQPLB-3"}},{"cell_type":"code","source":["# Function to evaluate the model on test data\n","def evaluate_model(model, data_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1)\n","            correct_predictions += (preds == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    accuracy = correct_predictions / total_predictions\n","    return accuracy\n","\n","# Evaluate the model on test data\n","test_accuracy = evaluate_model(model, test_data_loader)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"],"metadata":{"id":"xvGVK-iAxWRl","executionInfo":{"status":"ok","timestamp":1701072580323,"user_tz":-540,"elapsed":21224,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af79901a-b231-4d65-a47d-288cb2f9f681"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8610\n"]}]},{"cell_type":"markdown","source":["# Train (Mixed Precision)"],"metadata":{"id":"rsIFB-pkLP_r"}},{"cell_type":"code","source":["# Load model\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n","model = model.to(device)\n","\n","# Training loop\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","num_epochs = 1\n","total_steps = len(train_data_loader) * num_epochs\n","\n","# Initialize the gradient scaler for mixed precision\n","scaler = GradScaler()\n","\n","start_time = time.time()\n","\n","print(\"Starting training...\")\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for i, batch in enumerate(train_data_loader):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Enable autocast for the forward pass, automatically casting inputs to mixed precision\n","        with autocast():\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","        # Scales loss and calls backward() to create scaled gradients\n","        scaler.scale(loss).backward()\n","\n","        # Unscales the gradients and calls or skips optimizer.step()\n","        scaler.step(optimizer)\n","\n","        # Updates the scale for next iteration\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","        logits = outputs.logits\n","        preds = torch.argmax(logits, dim=1)\n","        correct_predictions += (preds == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","    epoch_loss = total_loss / len(train_data_loader)\n","    epoch_accuracy = correct_predictions / total_predictions\n","    print(f\"Epoch [{epoch+1}/{num_epochs}]: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","training_time = time.time() - start_time\n","print(f\"Training completed in {training_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoGWk8CYLJdm","executionInfo":{"status":"ok","timestamp":1701072795119,"user_tz":-540,"elapsed":169612,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"e3f33ea3-867c-46cc-a524-3b61ce172bca"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch [1/1]: Loss: 0.4755, Accuracy: 0.8330\n","Training completed in 167.17 seconds\n"]}]},{"cell_type":"markdown","source":["# Test (Mixed Precision)"],"metadata":{"id":"rCneo-9vOsx_"}},{"cell_type":"code","source":["# Function to evaluate the model on test data\n","def evaluate_model(model, data_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1)\n","            correct_predictions += (preds == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    accuracy = correct_predictions / total_predictions\n","    return accuracy\n","\n","# Evaluate the model on test data\n","test_accuracy = evaluate_model(model, test_data_loader)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2t24_LtMh9o","executionInfo":{"status":"ok","timestamp":1701072845664,"user_tz":-540,"elapsed":21696,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"8d8730a9-bd21-466d-da9a-c3f5f3373350"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.8610\n"]}]}]}